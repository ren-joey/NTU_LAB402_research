{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7778130,"sourceType":"datasetVersion","datasetId":4108739}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPU Status","metadata":{}},{"cell_type":"code","source":"# check GPU type.\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:08:59.660266Z","iopub.execute_input":"2024-03-06T19:08:59.661042Z","iopub.status.idle":"2024-03-06T19:09:00.758619Z","shell.execute_reply.started":"2024-03-06T19:08:59.661008Z","shell.execute_reply":"2024-03-06T19:09:00.757512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Python package installations","metadata":{}},{"cell_type":"code","source":"!pip install torchio\n!pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:00.760999Z","iopub.execute_input":"2024-03-06T19:09:00.761321Z","iopub.status.idle":"2024-03-06T19:09:29.116038Z","shell.execute_reply.started":"2024-03-06T19:09:00.761291Z","shell.execute_reply":"2024-03-06T19:09:29.114859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\nimport warnings\nfrom typing import Dict\nfrom pathlib import Path\nimport pandas as pd\nimport math\n\n# System packages\nimport os\nfrom os import listdir\nfrom os.path import splitext, isfile, join\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport torchio as tio\n\n# Visualization packages\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import normalize\n\nfrom argparse import ArgumentParser\n\nfrom timm.models.layers import trunc_normal_\nfrom cosine_annealing_warmup import CosineAnnealingWarmupRestarts","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-06T19:09:29.117681Z","iopub.execute_input":"2024-03-06T19:09:29.118016Z","iopub.status.idle":"2024-03-06T19:09:29.129288Z","shell.execute_reply.started":"2024-03-06T19:09:29.117985Z","shell.execute_reply":"2024-03-06T19:09:29.128239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class cfg:\n    seed = 77777777\n    \n    # Data\n    img_size = (512, 512)\n    in_channels = 1\n    \n    # 4 classes\n    n_classes = 4\n    train_img_path = '/kaggle/input/rtmets-sarcopenia/muscle_group_segment/train/xdata'\n    train_mask_path = '/kaggle/input/rtmets-sarcopenia/muscle_group_segment/train/ydata'\n    valid_img_path = '/kaggle/input/rtmets-sarcopenia/muscle_group_segment/validation/xdata'\n    valid_mask_path = '/kaggle/input/rtmets-sarcopenia/muscle_group_segment/validation/ydata'\n    all_img_path = '/kaggle/input/rtmets-sarcopenia/muscle_group_segment/all'\n    img_4c_path = '/kaggle/input/rtmets-sarcopenia/4_channels_sm'\n    \n    # Clinical Data\n    train_data_path = '/kaggle/input/rtmets-sarcopenia/RT_spine_NESMS_info/train.csv'\n    valid_data_path = '/kaggle/input/rtmets-sarcopenia/RT_spine_NESMS_info/valid.csv'\n    all_data_path = '/kaggle/input/rtmets-sarcopenia/RT_spine_NESMS_info/all.csv'\n    train_val_test_list_path = '/kaggle/input/rtmets-sarcopenia/train_val_test_split.csv'\n    pred_days = 42 # 42days, 90days, 365days\n    y_data_reverse = True\n    use_damper = True\n    \n    # Data Folding\n    cross_valid = 1\n\n    val_percent: float = 0.1\n    img_scale: float = 1.0\n    \n    # Transforms\n    scale = (0.7, 1.0)\n    \n    # Model args\n    bilinear = False\n    save_checkpoint: bool = True\n    amp: bool = False\n    checkpoint = '/kaggle/input/rtmets-sarcopenia/checkpoints/checkpoint_epoch48_0008.pth'\n    model_dir = './model'\n    \n    # Hyper parameters\n    patience: int = math.inf\n    epochs: int = 100\n    batch_size: int = 16\n    lr: float = 1e-3\n    weight_decay: float = 1e-1\n    momentum: float = 0.999\n    gradient_clipping: float = 1.0\n    optimizer = 'AdamW'\n    patience = 5\n    dropout = 0.4","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.132106Z","iopub.execute_input":"2024-03-06T19:09:29.132450Z","iopub.status.idle":"2024-03-06T19:09:29.146816Z","shell.execute_reply.started":"2024-03-06T19:09:29.132424Z","shell.execute_reply":"2024-03-06T19:09:29.145889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def seed_setting(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.enabled = False\n\ndef save_model(model_dir, model, args, epoch=None, k=None):\n    model_dir = Path(model_dir)\n    model_dir.mkdir(parents=True, exist_ok=True)\n    model_dir = model_dir.joinpath('{}d_fold{}_checkpoint_epoch{}.pth'.format(cfg.pred_days, k, epoch))\n\n    torch.save({\n        'epochs': args.epochs,\n        'model_state_dict': model.state_dict(),\n        'batch_size': args.batch_size,\n        'lr': args.lr,\n        'weight_decay': args.weight_decay,\n        'optimizer': args.optimizer,\n        'patience': args.patience,\n    }, model_dir)\n\ndef get_label_weights(args, k):\n#     weights = [3, 1]\n    weights = [1, 1]\n    print('weights = ', weights)\n    weights = torch.tensor(weights.copy(), dtype=torch.float32)\n    weights = weights.to(device)\n    return weights\n\n# all_scorer = sklearn.metrics.get_scorer_names()\n# print(all_scorer)\n\ndef evaluate_preds(y_test, preds, _preds):\n    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_test, _preds).ravel()\n    print(f'tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}')\n\n    acc = (tp+tn)/(tn+fp+fn+tp)\n    # acc = sklearn.metrics.accuracy_score(y_test, _preds)\n\n    sensitivity = tp/(tp+fn) # Recall\n    # sensitivity = sklearn.metrics.recall_score(y_test, _preds)\n\n    precision = tp/(tp+fp) # Precision & ppv\n    # precision = sklearn.metrics.precision_score(y_test, _preds)\n\n    npv = tn/(tn+fn)\n\n    specificity = tn/(tn+fp)\n\n    try:\n        f1 = ((2*precision*sensitivity)/(precision+sensitivity))\n    except:\n        f1 = 0\n    # f1 = sklearn.metrics.f1_score(y_test, _preds)\n\n    auc = sklearn.metrics.roc_auc_score(y_test, preds)\n\n#     fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, preds)\n# #     print(fpr)\n# #     print(tpr)\n# #     print(thresholds)\n#     roc_auc = sklearn.metrics.auc(fpr, tpr)\n#     display = sklearn.metrics.RocCurveDisplay(\n#         fpr=fpr,\n#         tpr=tpr,\n#         roc_auc=roc_auc\n#     )\n#     display.plot()\n#     plt.show()\n\n#     print(f'model_name: {cfg.model_name}')\n    print(f'acc: {acc}\\nsensitivity: {sensitivity}\\nspecificity: {specificity}\\nppv(precision): {precision}\\nnpv: {npv}\\nF1-score: {f1}\\nAUC: {auc}')\n    return auc","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.148088Z","iopub.execute_input":"2024-03-06T19:09:29.148629Z","iopub.status.idle":"2024-03-06T19:09:29.164922Z","shell.execute_reply.started":"2024-03-06T19:09:29.148598Z","shell.execute_reply":"2024-03-06T19:09:29.163949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparing","metadata":{}},{"cell_type":"code","source":"def data_preparing(data_path):\n    res = pd.read_csv(data_path)\n    data = res.values\n    header = res.columns.to_numpy()\n    \n    x_data, y_42d_data, y_90d_data, y_365d_data = data[:, :-3], data[:, -3], data[:, -2], data[:, -1]\n    \n    return x_data, y_42d_data, y_90d_data, y_365d_data, header\n\ndef distribution_map_preparing(train_val_test_list_path):\n    test = pd.read_csv(train_val_test_list_path, header=None)\n    data = test.values.tolist()\n    new_data = []\n    for each in data:\n        each = [i for i in each if str(i) != 'nan']\n        each = each[1:]\n        new_data.append(each)\n    return new_data","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.166488Z","iopub.execute_input":"2024-03-06T19:09:29.166861Z","iopub.status.idle":"2024-03-06T19:09:29.178113Z","shell.execute_reply.started":"2024-03-06T19:09:29.166827Z","shell.execute_reply":"2024-03-06T19:09:29.177334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"os.environ['KMP_DUPLICATE_LIB_OK']='True'\nos.environ['CUDA_LAUNCH_BLOCKING']='1'\n\n##### Dataset #####\nclass L3_Dataset(Dataset):\n    def __init__(self, images_path=None, clinical_data_path=None, train_val_test_list_path=None, mode=None, k=0, predict_mode=False, days=42, reverse=False):\n        self.images_path = images_path\n        self.clinical_data_path = clinical_data_path\n        self.mode = mode\n        self.predict_mode = predict_mode\n        split_map = distribution_map_preparing(train_val_test_list_path)\n        \n        x_data, y_42d_data, y_90d_data, y_365d_data, self.header = data_preparing(self.clinical_data_path)\n        self.x_data = x_data\n\n        if days == 42:\n            self.y_data = y_42d_data\n            base_idx = 0\n        elif days == 90:\n            self.y_data = y_90d_data\n            base_idx = 15\n        elif days == 365:\n            self.y_data = y_365d_data\n            base_idx = 30\n        else:\n            raise\n        \n        base_idx += k * 3\n        if mode == 'train':\n            self.split_map = split_map[base_idx] + split_map[base_idx + 1]\n        elif mode == 'val':\n            self.split_map = split_map[base_idx + 2]\n        else:\n            raise\n            \n        if reverse:\n            self.y_data = 1 - self.y_data\n        \n    def __len__(self):\n        return len(self.split_map)\n\n    def __getitem__(self, origin_idx):\n        idx = int(self.split_map[origin_idx] - 1)\n\n        img_path = Path(self.images_path, f'{idx + 1}.npy')\n#         mask_path = Path(self.images_path, 'ydata', f'{idx+1}.png')\n        img_name = img_path.stem\n\n        # label\n        label = self.y_data[idx]\n        label = int(label)\n        label = torch.tensor(label)\n\n        # Image\n        c4_img = np.load(img_path)\n        c4_img = c4_img.astype(np.float32)\n#         c4_img = np.zeros((512, 512, 4), dtype=np.float32)\n#         ct_img = np.array(Image.open(img_path))\n#         mask_img = np.array(Image.open(mask_path))\n#         for (y, rows) in enumerate(ct_img):\n#             for (x, p) in enumerate(rows):\n#                 [r, g, b] = mask_img[y][x]\n#                 c4_img[y][x] = [r, g, b, p]\n        c4_img = c4_img[:, :, :, np.newaxis].transpose((2, 3, 0, 1))\n\n        # clinical features\n        clinical_values = torch.tensor(self.x_data[idx], dtype=torch.float32)\n\n        return img_name, c4_img, clinical_values, label","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.179375Z","iopub.execute_input":"2024-03-06T19:09:29.179653Z","iopub.status.idle":"2024-03-06T19:09:29.194130Z","shell.execute_reply.started":"2024-03-06T19:09:29.179629Z","shell.execute_reply":"2024-03-06T19:09:29.193225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma=0, size_average=True, weights=None):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.size_average = size_average\n        self.weights = weights\n\n    def forward(self, input, target):\n        pt = F.softmax(input, dim=1)\n        logpt = F.log_softmax(input, dim=1)\n        weights = self.weights.index_select(0, target) # nice\n        target = target.view(-1, 1) # 1維 -> 2維\n\n        pt = pt.gather(1, target)\n        logpt = logpt.gather(1, target)\n        pt = pt.view(-1)\n        logpt = logpt.view(-1)\n\n        loss = -1 * weights * (1-pt)**self.gamma * logpt\n        if self.size_average: return loss.mean()\n        else: return loss.sum()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.195666Z","iopub.execute_input":"2024-03-06T19:09:29.195963Z","iopub.status.idle":"2024-03-06T19:09:29.208659Z","shell.execute_reply.started":"2024-03-06T19:09:29.195940Z","shell.execute_reply":"2024-03-06T19:09:29.207674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Blocks","metadata":{}},{"cell_type":"code","source":"class SE(nn.Module):\n    def __init__(self, channel, reduction=2):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1, 1)\n        return x * y.expand_as(x)\n\nclass GCT(nn.Module):\n    def __init__(self, channel, epsilon=1e-5, mode='l2', after_relu=False):\n        super(GCT, self).__init__()\n        self.alpha = nn.Parameter(torch.ones(1, channel, 1, 1, 1))\n        self.gamma = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.epsilon = epsilon\n        self.mode = mode\n        self.after_relu = after_relu\n\n    def forward(self, x):\n        if self.mode == 'l2':\n            embedding = (x.pow(2).sum((2, 3, 4), keepdim=True) + self.epsilon).pow(0.5) * self.alpha\n            norm = self.gamma / (embedding.pow(2).mean(dim=1, keepdim=True) + self.epsilon).pow(0.5)\n        else:\n            print('Unknown mode!')\n            sys.exit()\n\n        # gate = 1. + torch.tanh(embedding * norm + self.beta)\n        gate = 1. + torch.sigmoid(embedding * norm + self.beta)\n\n        return x * gate\n\nclass BAM(nn.Module):\n    def __init__(self, gate_channel, reduction=2, dilation_val=4, num_layers=1):\n        super(BAM, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        self.channel_att = nn.Sequential(\n            nn.Linear(gate_channel, gate_channel // reduction),\n            nn.ReLU(),\n            nn.Linear(gate_channel // reduction, gate_channel)\n        )\n\n        # input (_, _, D, H, W) -> output (_, _, D, H, W) for any Conv3D\n        self.spatial_att = nn.Sequential(\n            nn.Conv3d(gate_channel, gate_channel // reduction, kernel_size=1),\n            LayerNorm(gate_channel // reduction, data_format=\"channels_first\"),\n            nn.GELU(),\n            nn.Conv3d(gate_channel // reduction, gate_channel // reduction, kernel_size=3, padding=dilation_val, dilation=dilation_val),\n            LayerNorm(gate_channel // reduction, data_format=\"channels_first\"),\n            nn.GELU(),\n            nn.Conv3d(gate_channel // reduction, gate_channel // reduction, kernel_size=3, padding=dilation_val, dilation=dilation_val),\n            LayerNorm(gate_channel // reduction, data_format=\"channels_first\"),\n            nn.GELU(),\n            nn.Conv3d(gate_channel//reduction, 1, kernel_size=1)\n        )\n\n    def forward(self, x):\n        b, c, _, _, _ = x.size()\n        # compute channel attention\n        channel_part = self.avg_pool(x).view(b, c)\n        channel_part = self.channel_att(channel_part).view(b, c, 1, 1, 1).expand_as(x)\n        # compute spatial attention\n        spatial_part = self.spatial_att(x).expand_as(x)\n        # add together\n        att = 1 + F.sigmoid(channel_part + spatial_part)\n        return att * x\n\nclass CBAM(nn.Module):\n    def __init__(self, gate_channel, reduction=2):\n        super().__init__()\n        # channel attention\n        self.pools = [\n            nn.AdaptiveAvgPool3d(1),\n            nn.AdaptiveMaxPool3d(1)\n        ]\n        self.mlp = nn.Sequential(\n            nn.Linear(gate_channel, gate_channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(gate_channel // reduction, gate_channel)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n        # spatial attention\n        kernel_size = 7\n        self.conv = nn.Sequential(\n            nn.Conv3d(2, 1, kernel_size=kernel_size, stride=1, padding=(kernel_size-1) // 2),\n            LayerNorm(1, data_format=\"channels_first\"),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        res = x\n        b, c, _, _, _ = x.size()\n\n        channel_part = None\n        for pool in self.pools:\n            y = pool(x).view(b, c)\n            y = self.mlp(y)\n            channel_part = channel_part + y if channel_part is not None else y \n        channel_part = self.sigmoid(channel_part).view(b, c, 1, 1, 1)\n        x = x * channel_part.expand_as(x)\n\n        spatial_part = torch.cat( (torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1 )\n        spatial_part = self.conv(spatial_part)\n        x = x * spatial_part.expand_as(x)\n\n        return x + res\n\nclass SE_GCT(nn.Module):\n    def __init__(self, channel, reduction=2, epsilon=1e-5, mode='l2', after_relu=False):\n        super().__init__()\n        # Squeeze and excitation\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n        )\n        # GCT\n        self.alpha = nn.Parameter(torch.ones(1, channel, 1, 1, 1))\n        self.gamma = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.epsilon = epsilon\n        self.mode = mode\n        self.after_relu = after_relu\n\n    def forward(self, x):\n        # compute SE attention\n        b, c, _, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        se_gate = self.fc(y).view(b, c, 1, 1, 1).expand_as(x)\n\n        # compute GCT attention\n        if self.mode == 'l2':\n            embedding = (x.pow(2).sum((2, 3, 4), keepdim=True) + self.epsilon).pow(0.5) * self.alpha\n            norm = self.gamma / (embedding.pow(2).mean(dim=1, keepdim=True) + self.epsilon).pow(0.5)\n        else:\n            print('Unknown mode!')\n            sys.exit()\n        gct_gate = embedding * norm + self.beta\n\n        # add together\n        att = 1 + F.sigmoid(se_gate + gct_gate)\n        return att * x\n\nclass BAM_GCT(nn.Module):\n    def __init__(self, channel, reduction=2, dilation_val=4, num_layers=1, epsilon=1e-5, mode='l2', after_relu=False):\n        super().__init__()\n        # Squeeze and excitation\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n        )\n        # GCT\n        self.alpha = nn.Parameter(torch.ones(1, channel, 1, 1, 1))\n        self.gamma = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.epsilon = epsilon\n        self.mode = mode\n        self.after_relu = after_relu\n\n        # input (_, _, D, H, W) -> output (_, _, D, H, W) for any Conv3D\n        self.spatial_att = nn.Sequential(\n            nn.Conv3d(channel, channel // reduction, kernel_size=1),\n            LayerNorm(channel // reduction, data_format=\"channels_first\"),\n            nn.GELU(),\n            nn.Conv3d(channel // reduction, channel // reduction, kernel_size=3, padding=dilation_val, dilation=dilation_val),\n            LayerNorm(channel // reduction, data_format=\"channels_first\"),\n            nn.GELU(),\n            nn.Conv3d(channel // reduction, channel // reduction, kernel_size=3, padding=dilation_val, dilation=dilation_val),\n            LayerNorm(channel // reduction, data_format=\"channels_first\"),\n            nn.GELU(),\n            nn.Conv3d(channel//reduction, 1, kernel_size=1)\n        )\n\n    def forward(self, x):\n        b, c, _, _, _ = x.size()\n        # compute channel attention\n\n        # compute SE attention\n        b, c, _, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1, 1).expand_as(x)\n\n        # compute GCT attention\n        if self.mode == 'l2':\n            embedding = (x.pow(2).sum((2, 3, 4), keepdim=True) + self.epsilon).pow(0.5) * self.alpha\n            norm = self.gamma / (embedding.pow(2).mean(dim=1, keepdim=True) + self.epsilon).pow(0.5)\n        else:\n            print('Unknown mode!')\n            sys.exit()\n        gate = embedding * norm + self.beta\n\n        # add together\n        channel_part = y + gate\n\n        # compute spatial attention\n        spatial_part = self.spatial_att(x).expand_as(x)\n        # add together\n        att = 1 + F.sigmoid(channel_part + spatial_part)\n        return att * x\n\nclass CBAM_GCT(nn.Module):\n    def __init__(self, channel, reduction=2, epsilon=1e-5, mode='l2', after_relu=False):\n        super().__init__()\n        # channel attention\n        self.pools = [\n            nn.AdaptiveAvgPool3d(1),\n            nn.AdaptiveMaxPool3d(1)\n        ]\n        self.mlp = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n        # GCT\n        self.alpha = nn.Parameter(torch.ones(1, channel, 1, 1, 1))\n        self.gamma = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, channel, 1, 1, 1))\n        self.epsilon = epsilon\n        self.mode = mode\n        self.after_relu = after_relu\n\n        # spatial attention\n        kernel_size = 7\n        self.conv = nn.Sequential(\n            nn.Conv3d(2, 1, kernel_size=kernel_size, stride=1, padding=(kernel_size-1) // 2),\n            LayerNorm(1, data_format=\"channels_first\"),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        res = x\n        b, c, _, _, _ = x.size()\n\n        channel_part = None\n        for pool in self.pools:\n            y = pool(x).view(b, c)\n            y = self.mlp(y)\n            channel_part = channel_part + y if channel_part is not None else y \n        channel_part = channel_part.view(b, c, 1, 1, 1)\n\n        # compute GCT attention\n        if self.mode == 'l2':\n            embedding = (x.pow(2).sum((2, 3, 4), keepdim=True) + self.epsilon).pow(0.5) * self.alpha\n            norm = self.gamma / (embedding.pow(2).mean(dim=1, keepdim=True) + self.epsilon).pow(0.5)\n        else:\n            print('Unknown mode!')\n            sys.exit()\n        gate = embedding * norm + self.beta\n\n        # add together\n        channel_part = channel_part + gate\n        channel_part = F.sigmoid(channel_part)\n        x = x * channel_part.expand_as(x)\n\n        spatial_part = torch.cat( (torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1 )\n        spatial_part = self.conv(spatial_part)\n        x = x * spatial_part.expand_as(x)\n\n        return x + res\n\nclass LayerNorm(nn.Module):\n    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n    with shape (batch_size, channels, height, width).\n    \"\"\"\n    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.eps = eps\n        self.data_format = data_format\n        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n            raise NotImplementedError\n        self.normalized_shape = (normalized_shape, )\n    \n    def forward(self, x):\n        if self.data_format == \"channels_last\":\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        elif self.data_format == \"channels_first\":\n            u = x.mean(1, keepdim=True)\n            s = (x - u).pow(2).mean(1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.eps)\n            x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n            return x","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.210077Z","iopub.execute_input":"2024-03-06T19:09:29.210691Z","iopub.status.idle":"2024-03-06T19:09:29.276765Z","shell.execute_reply.started":"2024-03-06T19:09:29.210655Z","shell.execute_reply":"2024-03-06T19:09:29.275716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class GRN(nn.Module):\n    # gamma, beta: learnable affine transform parameters\n    # X: input of shape (N, D, H, W, C)\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.zeros((dim)))\n        self.beta = nn.Parameter(torch.zeros((dim)))\n\n    def forward(self, x):\n        gx = torch.norm(x, p=2, dim=(1, 2, 3), keepdim=True)\n        nx = gx / (gx.mean(dim=-1, keepdim=True) + 1e-6)\n        return self.gamma * (x * nx) + self.beta + x\n\nclass Block(nn.Module):\n    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n    (2) DwConv -> Permute to (N, D, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n    We use (2) as we find it slightly faster in PyTorch\n    \n    Args:\n        dim (int): Number of input channels.\n        drop_path (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n    def __init__(self, dim, depth_number: int, pos_in_depth: int, drop_path=0.5, layer_scale_init_value=1e-6):\n        super().__init__()\n        self.dwconv = nn.Conv3d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n                                    requires_grad=True) if layer_scale_init_value > 0 else None\n        self.depth_number = depth_number\n        self.pos_in_depth = pos_in_depth\n        self.se_gct = SE_GCT(channel=dim, reduction=2)\n        self.dim = dim\n\n    def forward(self, x):\n        input = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 4, 1) # (N, C, D, H, W) -> (N, D, H, W, C)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 4, 1, 2, 3) # (N, D, H, W, C) -> (N, C, D, H, W)\n\n        if self.pos_in_depth == 3:\n            x = self.se_gct(x)\n\n        x = input + x\n        return x\n\nclass ConvNeXt(nn.Module):\n    r\"\"\" ConvNeXt\n        A PyTorch impl of : `A ConvNet for the 2020s`  -\n          https://arxiv.org/pdf/2201.03545.pdf\n\n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n    \"\"\"\n    def __init__(\n            self, in_chans=4, num_classes=2, \n            depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n            layer_scale_init_value=1e-6, head_init_scale=1., len_of_clinical_features=71,\n            use_damper=False\n        ):\n        super().__init__()\n\n        # depths=[1, 1, 3, 1], dims=[8, 16, 32, 64]\n        self.use_damper = use_damper\n        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n        stem = nn.Sequential(\n            nn.Conv3d(in_chans, dims[0], kernel_size=(1, 4, 4), stride=4),\n            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                nn.Conv3d(dims[i], dims[i+1], kernel_size=(1, 2, 2), stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n        cur = 0\n        for i in range(4):\n            stage = nn.Sequential(\n                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], layer_scale_init_value=layer_scale_init_value, depth_number=i+1, pos_in_depth=j+1) for j in range(depths[i])]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n        \n        # voi size\n        self.v_fc1 = nn.Linear(len_of_clinical_features, 16)\n        self.v_fc2 = nn.Linear(16, dims[-1])\n        self.v_fc3 = nn.Linear(dims[-1], dims[-1])\n        \n        # final\n        len_of_features = dims[-1] + len_of_clinical_features\n        self.fc1 = nn.Linear(len_of_features, len_of_features // 2) # 214 = dim[-1] + dim(clinical_info) which is 150\n        self.fc2 = nn.Linear(len_of_features // 2, len_of_features // 4)\n        self.fc3 = nn.Linear(len_of_features // 4, 2)\n        self.bn1 = nn.BatchNorm1d(len_of_features // 2)\n        self.bn2 = nn.BatchNorm1d(len_of_features // 4)\n        self.softmax = nn.Softmax(dim=1)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv3d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02) # 正態分佈\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n        return self.norm(x.mean([-3, -2, -1])) # global average pooling, (N, C, D, H, W) -> (N, C)\n\n    def forward(self, x, clinical_info):\n        x_feature = self.forward_features(x)\n        \n        if self.use_damper:\n            voi_feature = self.v_fc1(clinical_info)\n            voi_feature = self.v_fc2(voi_feature)\n            voi_feature = self.v_fc3(voi_feature)\n\n            b, n = x_feature.size()\n            _sum = (x_feature * voi_feature).sum(dim=(-1))\n            x = x_feature - (_sum / n).view(b, 1).expand_as(x_feature)\n\n        x = torch.cat((x_feature, clinical_info), 1)\n        x = F.leaky_relu(self.bn1(self.fc1(x)))\n        x = F.leaky_relu(self.bn2(self.fc2(x)))\n        x = self.fc3(x)\n        x = self.softmax(x)\n\n        return x\n\ndef convnext_tiny(pretrained=False, in_22k=False, **kwargs):\n    # original dims = [96, 192, 384, 768]\n    # best dims = [8, 16, 32, 64]\n    model = ConvNeXt(depths=[1, 1, 3, 1], dims=[8, 16, 32, 64], **kwargs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.280677Z","iopub.execute_input":"2024-03-06T19:09:29.280989Z","iopub.status.idle":"2024-03-06T19:09:29.316689Z","shell.execute_reply.started":"2024-03-06T19:09:29.280963Z","shell.execute_reply":"2024-03-06T19:09:29.315640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Functions","metadata":{}},{"cell_type":"code","source":"def train(args, k: int, device, model, optimizer, criterion, scheduler, trainloader, valloader, seed):\n    \n    the_last_loss = 100\n    trigger_times = 0\n\n    the_best_loss = 0\n    the_best_accuracy = 0\n    \n    the_best_auc = 0\n\n    save_path = Path(args.model_dir)\n    save_tradoff_path = Path(args.model_dir, 'tradoff')\n    save_auc_path = Path(args.model_dir, 'auc')\n\n    for epoch in range(args.epochs):\n        epoch_loss, epoch_accuracy = 0, 0\n        train_label, train_pred = [], []\n        model.train()\n        for i, (_, img, clinical_info, label) in enumerate(trainloader):\n            img, clinical_info, label = img.to(device), clinical_info.to(device), label.to(device)\n\n            output = model(img, clinical_info)  #[bs, 2]\n\n            torch.cuda.empty_cache()\n\n            # _, pred = torch.max(output, dim=1)\n            pred = torch.where(output[:, 0] > 0.475, 0, 1)\n            # TODO: 不確定 label 有沒有二極化\n            loss = criterion(output, label)  #focal\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.detach().cpu().item()\n            epoch_accuracy += torch.sum(pred == label)\n\n            train_label += label.detach().cpu().numpy().tolist()\n            train_pred += pred.detach().cpu().numpy().tolist()\n\n        scheduler.step()\n\n        epoch_loss = epoch_loss / (len(trainloader) * args.batch_size)\n        epoch_accuracy = epoch_accuracy.float() / (len(trainloader) * args.batch_size)\n        tp, fn, fp, tn = confusion_matrix(train_label, train_pred).ravel()\n        tnr, fpr, fnr, tpr = tn/(fp+tn), fp/(fp+tn), fn/(tp+fn), tp/(tp+fn)\n        print(f'[{k}/{5}][{epoch}/{args.epochs}] loss: {epoch_loss:.8}, accuracy: {epoch_accuracy:.2}, tnr = {tnr:.2}, tpr = {tpr:.2}')\n\n        # Early stopping\n        the_current_loss, the_current_accuracy, the_current_tnr, the_current_tpr, the_current_auc = validation(device, model, criterion, valloader)\n        print(f'[validation] The current loss: {the_current_loss:.8}, accuracy: {the_current_accuracy:.2}, tnr = {the_current_tnr:.2}, tpr = {the_current_tpr:.2}')\n\n        if the_current_loss > the_last_loss:\n            trigger_times += 1\n            print('trigger times:', trigger_times)\n\n            if trigger_times >= args.patience:\n                print(f'Early stopping!\\nEpoch = {epoch}')\n                return\n        else:\n            print('trigger times: 0')\n            trigger_times = 0\n\n#         if epoch == 0 or the_best_loss >= the_current_loss:\n#             print(f'Recording best model. ({save_path})')\n#             save_model(save_path, model, args, epoch, k)\n#             the_best_loss = the_current_loss\n\n        if epoch == 0 or (the_best_accuracy <= the_current_accuracy and abs(the_current_tnr-the_current_tpr) <= 0.15):\n            print(f'Recording best tradeoff model. ({save_tradoff_path})')\n            save_model(save_tradoff_path, model, args, epoch, k)\n            the_best_accuracy = the_current_accuracy\n        \n        if the_current_auc > the_best_auc:\n            print(f'Recording best auc model. ({save_auc_path})')\n            save_model(save_auc_path, model, args, epoch, k)\n            the_best_auc = the_current_auc\n\n        the_last_loss = the_current_loss\n\n    print(f'stopping! Epoch = {args.epochs}')\n    return \n\ndef validation(device, model, criterion, valloader):\n    model.eval()\n    val_loss = 0\n    val_accuracy = 0\n    val_label, val_pred, val_c0 = [], [], []\n\n    with torch.no_grad():\n        for _, img, clinical_info, label in valloader:\n            img, clinical_info, label = img.to(device), clinical_info.to(device), label.to(device)\n            output = model(img, clinical_info)  #[bs, 2]\n            \n            # _, pred = torch.max(output, dim=1)\n            c0 = output[:, 0]\n            pred = torch.where(output[:, 0] > 0.475, 0, 1)\n            loss = criterion(output, label)  #focal\n            val_loss += loss.detach().cpu().item()\n            val_accuracy += torch.sum(pred == label)\n\n            val_label += label.detach().cpu().numpy().tolist()\n            val_pred += pred.detach().cpu().numpy().tolist()\n            val_c0 += c0.detach().cpu().numpy().tolist()\n\n    auc = evaluate_preds(val_label, val_c0, val_pred)\n\n    val_loss = val_loss / len(valloader)\n    val_accuracy = val_accuracy.float() / len(valloader)\n    tp, fn, fp, tn = confusion_matrix(val_label, val_pred).ravel()\n    tnr, fpr, fnr, tpr = tn/(fp+tn), fp/(fp+tn), fn/(tp+fn), tp/(tp+fn)\n            \n    return val_loss, val_accuracy, tnr, tpr, auc","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.318093Z","iopub.execute_input":"2024-03-06T19:09:29.318403Z","iopub.status.idle":"2024-03-06T19:09:29.344620Z","shell.execute_reply.started":"2024-03-06T19:09:29.318378Z","shell.execute_reply":"2024-03-06T19:09:29.343652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Arguments Preparing","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start Training","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__' :\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"device = \", device)\n\n    for k in range(1, 6):\n        ##### Setting Seed #####\n        seed = cfg.seed\n        seed_setting(seed)\n\n        model = convnext_tiny(len_of_clinical_features=71, use_damper=cfg.use_damper)\n        model.to(device)\n\n        weights = get_label_weights(cfg, k)\n        criterion = FocalLoss(gamma=2, weights=weights).to(device)\n        optimizer = getattr(optim, cfg.optimizer)(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n\n        scheduler = CosineAnnealingWarmupRestarts(\n            optimizer,\n            first_cycle_steps=20,\n            cycle_mult=1.0,\n            max_lr=cfg.lr,\n            min_lr=0.000001,\n            warmup_steps=10,\n            gamma=1.0,\n        )\n        # Data\n        trainset = L3_Dataset(\n            images_path=cfg.img_4c_path, \n            clinical_data_path=cfg.all_data_path,\n            train_val_test_list_path=cfg.train_val_test_list_path,\n            mode=\"train\",\n            k=k,\n#             days=days,\n            days=cfg.pred_days,\n            reverse=cfg.y_data_reverse\n        )\n        trainloader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n        valset = L3_Dataset(\n            images_path=cfg.img_4c_path, \n            clinical_data_path=cfg.all_data_path,\n            train_val_test_list_path=cfg.train_val_test_list_path,\n            mode=\"val\",\n            k=k,\n#             days=days,\n            days=cfg.pred_days,\n            reverse=cfg.y_data_reverse\n        )\n        valloader = DataLoader(valset, batch_size=1, shuffle=False)\n\n        train(cfg, k, device, model, optimizer, criterion, scheduler, trainloader, valloader, seed)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T19:09:29.346083Z","iopub.execute_input":"2024-03-06T19:09:29.346477Z","iopub.status.idle":"2024-03-06T19:09:51.465337Z","shell.execute_reply.started":"2024-03-06T19:09:29.346443Z","shell.execute_reply":"2024-03-06T19:09:51.463539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}