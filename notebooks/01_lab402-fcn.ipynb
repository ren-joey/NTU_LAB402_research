{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7874184,"sourceType":"datasetVersion","datasetId":4108739}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-18T10:43:49.745784Z","iopub.execute_input":"2024-03-18T10:43:49.746848Z","iopub.status.idle":"2024-03-18T10:43:50.794555Z","shell.execute_reply.started":"2024-03-18T10:43:49.746800Z","shell.execute_reply":"2024-03-18T10:43:50.793463Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Mon Mar 18 10:43:50 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0              31W / 250W |    326MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:43:50.796746Z","iopub.execute_input":"2024-03-18T10:43:50.797107Z","iopub.status.idle":"2024-03-18T10:44:03.531971Z","shell.execute_reply.started":"2024-03-18T10:43:50.797065Z","shell.execute_reply":"2024-03-18T10:44:03.530799Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.4.1.post1)\nRequirement already satisfied: numpy<2.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.4)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Numerical Operations\nimport math\nimport numpy as np\n\n# Reading/Writing Data\nimport pandas as pd\nimport os\nimport csv\n\n# For Progress Bar\nfrom tqdm import tqdm\n\n# Pytorch\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# For plotting learning curve\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport sklearn\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.533660Z","iopub.execute_input":"2024-03-18T10:44:03.533978Z","iopub.status.idle":"2024-03-18T10:44:03.540755Z","shell.execute_reply.started":"2024-03-18T10:44:03.533947Z","shell.execute_reply":"2024-03-18T10:44:03.539754Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nclass cfg:\n    seed = 5201314      # Your seed number, you can pick your lucky number. :)\n    select_all = True   # Whether to use all features.\n    valid_ratio = 0.1   # validation_size = train_size * valid_ratio\n    n_epochs = 3000     # Number of epochs.            \n    batch_size = 64\n    lr_lambda = 0.996\n    optimizer = 'Adam'\n    early_stop = 600  # 600   # If model has not improved for this many consecutive epochs, stop training.     \n    train_path = '/kaggle/input/rtmets-sarcopenia/RT_spine_NESMS_info/train.csv'\n    valid_path = '/kaggle/input/rtmets-sarcopenia/RT_spine_NESMS_info/valid.csv'\n    all_data_path = '/kaggle/input/rtmets-sarcopenia/RT_spine_NESMS_info/all.csv'\n    train_val_test_list_path = '/kaggle/input/rtmets-sarcopenia/train_val_test_split.csv'\n\n    radiomics_idx = 7\n    pred_days = 42 # 42, 90, 365\n    y_data_reverse = True\n    data_using = 'radiomics' # 'clinical', 'radiomics', 'all'\n    data_sampling = 'no-sampling' # 'no-sampling' | 'over-sampling' | 'under-sampling'\n    \n    # hyper-parameters for the optimizer (depends on which optimizer you are using)\n    opt_params = {\n        'lr': 1e-2,                        # learning rate of Adam\n#         'momentum': 0.9,\n        'weight_decay': 5e-3              # weight decay of Adam   \n    }\n    \n    assert pred_days in (42, 90, 365)\n    assert data_using in ('clinical', 'radiomics', 'all')\n\n    post_fix = '' if select_all else 'selected'\n    model_name = f'{pred_days}d_{post_fix}_{data_sampling}_{data_using}_model'\n    save_path = f'./models/'  # Your model will be saved here.\n    pred_path = f'./preds/{model_name}_pred.csv'\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.543749Z","iopub.execute_input":"2024-03-18T10:44:03.544158Z","iopub.status.idle":"2024-03-18T10:44:03.556011Z","shell.execute_reply.started":"2024-03-18T10:44:03.544133Z","shell.execute_reply":"2024-03-18T10:44:03.555233Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def same_seed(seed): \n    '''Fixes random number generator seeds for reproducibility.'''\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef predict(test_loader, model, device):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    for x in tqdm(test_loader):\n        x = x.to(device)                        \n        with torch.no_grad():                   \n            pred = model(x)                     \n            preds.append(pred.detach().cpu())   \n    preds = torch.cat(preds, dim=0).numpy()  \n    return preds\n\ndef plot_learning_curve(loss_record, title=''):\n    ''' Plot learning curve of your DNN (train & dev loss) '''\n    total_steps = len(loss_record['train'])\n    x_1 = range(total_steps)\n    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n    figure(figsize=(6, 4))\n    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n    plt.ylim(0.0, 5.)\n    plt.xlabel('Training steps')\n    plt.ylabel('MSE loss')\n    plt.title('Learning curve of {}'.format(title))\n    plt.legend()\n    plt.show()\n\ndef evaluate_preds(y_test, preds, threshold=0.5):\n    _preds = []\n    for i, v in enumerate(preds):\n        _preds.append(1 if (v > threshold) else 0)\n\n    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_test, _preds).ravel()\n#     print(f'tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}')\n\n    acc = (tp+tn)/(tn+fp+fn+tp)\n    # acc = sklearn.metrics.accuracy_score(y_test, _preds)\n\n    tpr = tp/(tp+fn) # Recall, tpr, sensitivity\n    # sensitivity = sklearn.metrics.recall_score(y_test, _preds)\n\n    tnr = tn/(tn+fp) # tnr, specificity\n    \n    ppv = tp/(tp+fp) # Precision & ppv\n    # precision = sklearn.metrics.precision_score(y_test, _preds)\n\n    npv = tn/(tn+fn)\n    \n#     try:\n#         f1 = ((2*ppv*tpr)/(ppv+tpr))\n#     except:\n#         f1 = 0\n    # f1 = sklearn.metrics.f1_score(y_test, _preds)\n\n    y_test = 1 - np.array(y_test)\n    auc = sklearn.metrics.roc_auc_score(y_test, preds)\n    auc = 1 - auc if auc < 0.5 else auc\n    \n#     fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, preds)\n#     roc_auc = sklearn.metrics.auc(fpr, tpr)\n#     display = sklearn.metrics.RocCurveDisplay(\n#         fpr=fpr,\n#         tpr=tpr,\n#         roc_auc=roc_auc\n#     )\n#     display.plot()\n#     plt.show()\n\n    print(f'acc: {acc:.4}, tpr: {tpr:.4}, tnr: {tnr:.4}, ppv: {ppv:.4}, npv: {npv:.4}, auc: {auc:.4}')\n\n    return acc, auc\n\ndef save_pred(preds, file, label=None):\n    ''' Save predictions to specified file '''\n    with open(file, 'w') as fp:\n        writer = csv.writer(fp)\n        header = ['id', 'pred'] if label is None else ['id', 'pred', 'label']\n        writer.writerow(header)\n        for i, p in enumerate(preds):\n            if label is None:\n                writer.writerow([i, p])\n            else:\n                writer.writerow([i, p, label[i]])","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.557209Z","iopub.execute_input":"2024-03-18T10:44:03.557591Z","iopub.status.idle":"2024-03-18T10:44:03.578002Z","shell.execute_reply.started":"2024-03-18T10:44:03.557559Z","shell.execute_reply":"2024-03-18T10:44:03.577173Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def data_preparing(data_path):\n    res = pd.read_csv(data_path)\n    data = res.values\n    header = res.columns.to_numpy()\n    \n    x_data, y_42d_data, y_90d_data, y_365d_data = data[:, :-3], data[:, -3], data[:, -2], data[:, -1]\n    \n    return x_data, y_42d_data, y_90d_data, y_365d_data, header\n\ndef distribution_map_preparing(train_val_test_list_path):\n    test = pd.read_csv(train_val_test_list_path, header=None)\n    data = test.values.tolist()\n    new_data = []\n    for each in data:\n        each = [i for i in each if str(i) != 'nan']\n        each = each[1:]\n        new_data.append(each)\n    return new_data","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.579090Z","iopub.execute_input":"2024-03-18T10:44:03.579425Z","iopub.status.idle":"2024-03-18T10:44:03.592236Z","shell.execute_reply.started":"2024-03-18T10:44:03.579393Z","shell.execute_reply":"2024-03-18T10:44:03.591377Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class RTDataset(Dataset):\n    '''\n    x: Features.\n    y: Targets, if none, do prediction.\n    '''\n    def __init__(self,\n                 sampling,\n                 days,\n                 k,\n                 train_val_test_list_path,\n                 clinical_data_path,\n                 radiomics_idx,\n                 data_using,\n                 mode,\n                 reverse\n                ):\n        assert sampling in ('over-sampling', 'under-sampling', 'no-sampling')\n        assert days in (42, 90, 365)\n        \n        print(f'Dataset Properties:\\nSampling:{sampling}\\nDays:{days}\\nF:{k}\\nData_using:{data_using}\\nMode:{mode}')\n        \n        split_map = distribution_map_preparing(train_val_test_list_path)\n        x_data, y_42d_data, y_90d_data, y_365d_data, self.header = data_preparing(clinical_data_path)\n        \n        if data_using == 'clinical':\n            self.x_data = x_data[:, radiomics_idx:]\n        elif data_using == 'radiomics':\n            self.x_data = x_data[:, :radiomics_idx]\n        \n        if days == 42:\n            self.y_data = y_42d_data\n            base_idx = 0\n        elif days == 90:\n            self.y_data = y_90d_data\n            base_idx = 15\n        elif days == 365:\n            self.y_data = y_365d_data\n            base_idx = 30\n        else:\n            raise\n        \n        if sampling == 'under-sampling':\n            base_idx += 45\n        elif sampling == 'no-sampling':\n            base_idx += 90\n            \n        base_idx += k * 3\n        \n        if mode == 'train':\n            self.split_map = split_map[base_idx] + split_map[base_idx + 1]\n        elif mode == 'val':\n            self.split_map = split_map[base_idx + 2]\n        else:\n            raise\n            \n        if reverse:\n            self.y_data = 1 - self.y_data\n\n    def __getitem__(self, origin_idx):\n        idx = int(self.split_map[origin_idx] - 1)\n        clinical_values = torch.tensor(self.x_data[idx], dtype=torch.float32)\n\n        # label\n        label = self.y_data[idx]\n        label = torch.tensor(label, dtype=torch.float)\n        \n        return clinical_values, label\n\n    def __len__(self):\n        return len(self.split_map)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.594873Z","iopub.execute_input":"2024-03-18T10:44:03.595279Z","iopub.status.idle":"2024-03-18T10:44:03.606775Z","shell.execute_reply.started":"2024-03-18T10:44:03.595247Z","shell.execute_reply":"2024-03-18T10:44:03.606006Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        # TODO: modify model's structure, be aware of dimensions. \n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 64),\n#             nn.Dropout(p=0.5),\n#             nn.ReLU(),\n#             nn.Linear(128, 64),\n#             nn.Dropout(p=0.4),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n#             nn.Dropout(p=0.3),\n            nn.ReLU(),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1)\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.squeeze(1) # (B, 1) -> (B)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.607760Z","iopub.execute_input":"2024-03-18T10:44:03.608003Z","iopub.status.idle":"2024-03-18T10:44:03.620794Z","shell.execute_reply.started":"2024-03-18T10:44:03.607981Z","shell.execute_reply":"2024-03-18T10:44:03.620089Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"def feature_selector(features, targets, header=None):\n    kbest = SelectKBest(score_func=f_regression, k=30)\n    fit = kbest.fit(features, targets)\n    \n    if header is not None:\n#         print(\n#             { header[i]: kbest.scores_[i] for i in range(len(header)) }\n#         )\n        dfscores = pd.DataFrame(fit.scores_)\n        dfcolumns = pd.DataFrame(header)\n\n        # concat two dataframes for better visualization \n        featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n        featureScores.columns = ['Specs', 'Score']  # naming the dataframe columns\n        largest_30 = featureScores.nlargest(30, 'Score')\n        print(largest_30)\n        return largest_30.index.to_numpy()\n    \n    return np.array([])","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.622119Z","iopub.execute_input":"2024-03-18T10:44:03.622729Z","iopub.status.idle":"2024-03-18T10:44:03.634239Z","shell.execute_reply.started":"2024-03-18T10:44:03.622696Z","shell.execute_reply":"2024-03-18T10:44:03.633407Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def train_valid_split(data_set, valid_ratio, seed):\n    '''Split provided training data into training set and validation set'''\n    valid_set_size = int(valid_ratio * len(data_set)) \n    train_set_size = len(data_set) - valid_set_size\n    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n    return np.array(train_set), np.array(valid_set)\n\ndef data_reverse(y_data):\n    return 1 - y_data\n\ndef select_feat(train_data, test_data, valid_ratio, seed, select_all=True, days=42, y_data_reverse=False, cross_valid=1):\n    if days == 42:\n        y_idx = -3\n    elif days == 90:\n        y_idx = -2\n    elif days == 365:\n        y_idx = -1\n    else:\n        raise\n\n    if (cross_valid == 1):\n        '''Selects useful features to perform regression'''\n        train_data, valid_data = train_valid_split(train_data, valid_ratio, seed)\n        \n        # Print out the data size.\n        print(f\"\"\"train_data size: {train_data.shape} \n        valid_data size: {valid_data.shape} \n        test_data size: {test_data.shape}\"\"\")\n        \n        y_train, y_valid, y_test = train_data[:,y_idx], valid_data[:,y_idx], test_data[:, y_idx]\n        raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-3], valid_data[:,:-3], test_data[:,:-3]\n\n        if y_data_reverse:\n            y_train, y_valid, y_test = data_reverse(y_train), data_reverse(y_valid), data_reverse(y_test)\n\n        if select_all:\n            feat_idx = list(range(raw_x_train.shape[1]))\n        else:\n    #         feat_idx = list(range(35, raw_x_train.shape[1])) # TODO: Select suitable feature columns.\n    #         pri_feats = [35, 36, 47, 48, 52]\n    #         feat_idx = [35, 36, 47, 48, 52] + [i + 18 for i in pri_feats] + [i + 36 for i in pri_feats[:-1]]\n            feat_idx = [5, 6, 7, 9, 11, 14, 19, 20, 24, 25, 29, 41, 58, 67, 68]\n            \n        return [[raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid, y_test]]\n    else:\n        data = np.concatenate((train_data, test_data), axis=0)\n        fold_size = data.shape[0] // cross_valid\n        folds = []\n        for i in range(cross_valid):\n            _data = data.copy()\n            fold_1 = [fold_size * i, fold_size * (i + 1)]\n            fold_2 = [fold_size * (i + 1), fold_size * (i + 2)] if i < (cross_valid - 1) else [0, fold_size * (i + 1)]\n            valid = _data[: fold_1]\n            test = _data[fold_1: fold_2]\n            train = _data[:fold_1[0]] + (_data[:fold_2[1]] if i < (cross_valid - 1) else _data[fold_1[1]:])\n            y_train, y_valid, y_test = train[:, y_idx], valid[:, y_idx], test[:, y_idx]\n            raw_x_train, raw_x_valid, raw_x_test = train[:, :-3], valid[:, :-3], test[:, :-3]\n            folds.append([raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid, y_test])\n\n        return folds","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.637879Z","iopub.execute_input":"2024-03-18T10:44:03.638427Z","iopub.status.idle":"2024-03-18T10:44:03.655950Z","shell.execute_reply.started":"2024-03-18T10:44:03.638401Z","shell.execute_reply":"2024-03-18T10:44:03.655075Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def trainer(train_loader, valid_loader, model, cfg, device, k, model_name):\n    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n\n    # Define your optimization algorithm. \n    # TODO: Please check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n    # TODO: L2 regularization (optimizer(weight decay...) or implement by your self).\n    # optimizer = torch.optim.RMSprop(model.parameters(), lr=cfg.learning_rate, momentum=0.9) \n    # optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate) \n    # Setup optimizer\n    optimizer = getattr(torch.optim, cfg.optimizer)(model.parameters(), **cfg.opt_params)\n    writer = SummaryWriter() # Writer of tensoboard.\n    lambda1 = lambda epoch: cfg.lr_lambda ** epoch\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n\n    if not os.path.isdir('./models'):\n        os.mkdir('./models') # Create directory of saving models.\n\n    n_epochs, best_loss, step, early_stop_count, best_auc = cfg.n_epochs, math.inf, 0, 0, 0\n\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n\n        # tqdm is a package to visualize your training progress.\n        # train_pbar = tqdm(train_loader, position=0, leave=True)\n        \n        for x, y in train_loader:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x)\n            loss = criterion(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # Display current epoch number and loss on tqdm progress bar.\n            # train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n            # train_pbar.set_postfix({'loss': loss.detach().item()})\n\n        scheduler.step()\n        mean_train_loss = sum(loss_record)/len(loss_record)\n        writer.add_scalar('Loss/train', mean_train_loss, step)\n\n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        \n        labels = []\n        preds = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = criterion(pred, y)\n                preds += pred.detach().cpu().numpy().tolist()\n                labels += y.detach().cpu().numpy().tolist()\n\n            loss_record.append(loss.item())\n        \n        acc, auc = evaluate_preds(labels, preds)\n        \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        print(f'Fold{k+1} Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n       \n        # Save your best model\n        if auc > best_auc:\n            best_auc = auc\n            torch.save(model.state_dict(), cfg.save_path + '{}_{}_auc{:.4}.ckpt'.format(model_name, epoch, auc))\n            save_pred(preds, cfg.save_path + '{}_f{}_{}_auc{:.4}.csv'.format(model_name, k+1, epoch, auc), labels)\n            print('Fold{} Saving model with auc {:.3f}...'.format(k+1, best_auc))\n\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), cfg.save_path + '{}_{}_acc{:.4}.ckpt'.format(model_name, epoch, auc))\n            save_pred(preds, cfg.save_path + '{}_f{}_{}_acc{:.4}.csv'.format(model_name, k+1, epoch, auc), labels)\n            print('Fold{} Saving model with loss {:.3f}...'.format(k+1, best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n\n        if early_stop_count >= cfg.early_stop:\n            print('\\nModel is not improving, so we halt the training session.')\n            return","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.657341Z","iopub.execute_input":"2024-03-18T10:44:03.657634Z","iopub.status.idle":"2024-03-18T10:44:03.676481Z","shell.execute_reply.started":"2024-03-18T10:44:03.657610Z","shell.execute_reply":"2024-03-18T10:44:03.675530Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Set seed for reproducibility\nsame_seed(cfg.seed)\n\n# train_data size: 3009 x 89 (35 states + 18 features x 3 days) \n# test_data size: 997 x 88 (without last day's positive rate)\n# train_data, test_data = pd.read_csv(cfg.train_path).values, pd.read_csv(cfg.valid_path).values\n# header = pd.read_csv(cfg.train_path).columns.to_numpy()\n\n# Select features\n# x_train, x_valid, x_test, y_train, y_valid, y_test\n# folds = select_feat(\n#     train_data,\n#     test_data,\n#     cfg.valid_ratio,\n#     cfg.seed,\n#     cfg.select_all,\n#     cfg.pred_days,\n#     cfg.y_data_reverse,\n#     cfg.cross_valid\n# )\n\n# [x_train, x_valid, x_test, y_train, y_valid, y_test] = folds[0]\n\n# Evaluate features' KBest\n# feature_selector(x_train, y_train, header)\n\n# Print out the number of features.\n\n# train_dataset, valid_dataset, test_dataset = RTDataset(x_train, y_train), \\\n#                                             RTDataset(x_valid, y_valid), \\\n#                                             RTDataset(x_test)\n# print(f'number of features: {len(train_dataset.__getitem__(0)[0])}')\n\n# Pytorch data loader loads pytorch dataset into batches.\n# train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, pin_memory=True)\n# valid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=True, pin_memory=True)\n# test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.677799Z","iopub.execute_input":"2024-03-18T10:44:03.678242Z","iopub.status.idle":"2024-03-18T10:44:03.690405Z","shell.execute_reply.started":"2024-03-18T10:44:03.678209Z","shell.execute_reply":"2024-03-18T10:44:03.689668Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"for days in [42, 90, 365]:\n    for data_using in ['clinical', 'radiomics']:\n        for k in range(5):\n            model_name = f'{days}d_{cfg.post_fix}_{cfg.data_sampling}_{data_using}_model'\n            print(f'model name: {model_name}\\n')\n            \n            # Data\n            trainset = RTDataset(\n                clinical_data_path=cfg.all_data_path,\n                train_val_test_list_path=cfg.train_val_test_list_path,\n                mode=\"train\",\n                k=k,\n#                 days=cfg.pred_days,\n                days=days,\n                reverse=cfg.y_data_reverse,\n                sampling=cfg.data_sampling,\n                data_using=data_using,\n                radiomics_idx=cfg.radiomics_idx\n            )\n            print('training set size: {}\\n'.format(trainset.__len__()))\n            trainloader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n\n            valset = RTDataset(\n                clinical_data_path=cfg.all_data_path,\n                train_val_test_list_path=cfg.train_val_test_list_path,\n                mode=\"val\",\n                k=k,\n#                 days=cfg.pred_days,\n                days=days,\n                reverse=cfg.y_data_reverse,\n                sampling=cfg.data_sampling,\n                data_using=data_using,\n                radiomics_idx=cfg.radiomics_idx\n            )\n            print('val set size: {}\\n'.format(valset.__len__()))\n            valloader = DataLoader(valset, batch_size=cfg.batch_size, shuffle=False)\n\n            model = Model(input_dim=len(trainset.__getitem__(0)[0])).to(device) # put your model and data on the same computation device.\n            trainer(trainloader, valloader, model, cfg, device, k, model_name)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:03.691473Z","iopub.execute_input":"2024-03-18T10:44:03.691757Z","iopub.status.idle":"2024-03-18T10:44:14.616954Z","shell.execute_reply.started":"2024-03-18T10:44:03.691734Z","shell.execute_reply":"2024-03-18T10:44:14.615576Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"model name: 42d__no-sampling_clinical_model\n\nDataset Properties:\nSampling:no-sampling\nDays:42\nF:0\nData_using:clinical\nMode:train\ntraining set size: 730\n\nDataset Properties:\nSampling:no-sampling\nDays:42\nF:0\nData_using:clinical\nMode:val\nval set size: 181\n\nacc: 0.7127, tpr: 0.4, tnr: 0.7628, ppv: 0.2128, npv: 0.8881, auc: 0.6377\nFold1 Epoch [1/3000]: Train loss: 3.8605, Valid loss: 0.4809\nFold1 Saving model with auc 0.638...\nFold1 Saving model with loss 0.481...\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.5756\nFold1 Epoch [2/3000]: Train loss: 0.2628, Valid loss: 0.2637\nFold1 Saving model with loss 0.264...\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.621\nFold1 Epoch [3/3000]: Train loss: 0.1587, Valid loss: 0.1203\nFold1 Saving model with loss 0.120...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n","output_type":"stream"},{"name":"stdout","text":"acc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.5205\nFold1 Epoch [4/3000]: Train loss: 0.1347, Valid loss: 0.1283\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.5592\nFold1 Epoch [5/3000]: Train loss: 0.1331, Valid loss: 0.1203\nFold1 Saving model with loss 0.120...\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.5031\nFold1 Epoch [6/3000]: Train loss: 0.1210, Valid loss: 0.1398\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.5623\nFold1 Epoch [7/3000]: Train loss: 0.1264, Valid loss: 0.1211\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.5349\nFold1 Epoch [8/3000]: Train loss: 0.1165, Valid loss: 0.1212\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n","output_type":"stream"},{"name":"stdout","text":"acc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.5638\nFold1 Epoch [9/3000]: Train loss: 0.1191, Valid loss: 0.1185\nFold1 Saving model with loss 0.118...\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.579\nFold1 Epoch [10/3000]: Train loss: 0.1149, Valid loss: 0.1188\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.5764\nFold1 Epoch [11/3000]: Train loss: 0.1124, Valid loss: 0.1201\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.6136\nFold1 Epoch [12/3000]: Train loss: 0.1120, Valid loss: 0.1170\nFold1 Saving model with loss 0.117...\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.5823\nFold1 Epoch [13/3000]: Train loss: 0.1110, Valid loss: 0.1293\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.6446\nFold1 Epoch [14/3000]: Train loss: 0.1062, Valid loss: 0.1137\nFold1 Saving model with auc 0.645...\nFold1 Saving model with loss 0.114...\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.6179\nFold1 Epoch [15/3000]: Train loss: 0.1113, Valid loss: 0.1238\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.5782\nFold1 Epoch [16/3000]: Train loss: 0.1143, Valid loss: 0.1240\nacc: 0.8508, tpr: 0.0, tnr: 0.9872, ppv: 0.0, npv: 0.8603, auc: 0.6713\nFold1 Epoch [17/3000]: Train loss: 0.1128, Valid loss: 0.1144\nFold1 Saving model with auc 0.671...\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.6905\nFold1 Epoch [18/3000]: Train loss: 0.1202, Valid loss: 0.1125\nFold1 Saving model with auc 0.691...\nFold1 Saving model with loss 0.113...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n","output_type":"stream"},{"name":"stdout","text":"acc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.6515\nFold1 Epoch [19/3000]: Train loss: 0.1061, Valid loss: 0.1160\nacc: 0.8508, tpr: 0.04, tnr: 0.9808, ppv: 0.25, npv: 0.8644, auc: 0.68\nFold1 Epoch [20/3000]: Train loss: 0.1058, Valid loss: 0.1167\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.7049\nFold1 Epoch [21/3000]: Train loss: 0.1157, Valid loss: 0.1109\nFold1 Saving model with auc 0.705...\nFold1 Saving model with loss 0.111...\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.6708\nFold1 Epoch [22/3000]: Train loss: 0.1074, Valid loss: 0.1266\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.6867\nFold1 Epoch [23/3000]: Train loss: 0.1111, Valid loss: 0.1111\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n","output_type":"stream"},{"name":"stdout","text":"acc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7033\nFold1 Epoch [24/3000]: Train loss: 0.1098, Valid loss: 0.1164\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.7141\nFold1 Epoch [25/3000]: Train loss: 0.1092, Valid loss: 0.1090\nFold1 Saving model with auc 0.714...\nFold1 Saving model with loss 0.109...\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.6713\nFold1 Epoch [26/3000]: Train loss: 0.1006, Valid loss: 0.1130\nacc: 0.8619, tpr: 0.0, tnr: 1.0, ppv: nan, npv: 0.8619, auc: 0.6985\nFold1 Epoch [27/3000]: Train loss: 0.1012, Valid loss: 0.1277\nacc: 0.8564, tpr: 0.0, tnr: 0.9936, ppv: 0.0, npv: 0.8611, auc: 0.7387\nFold1 Epoch [28/3000]: Train loss: 0.1092, Valid loss: 0.1062\nFold1 Saving model with auc 0.739...\nFold1 Saving model with loss 0.106...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2743759009.py:52: RuntimeWarning: invalid value encountered in scalar divide\n  ppv = tp/(tp+fp) # Precision & ppv\n","output_type":"stream"},{"name":"stdout","text":"acc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.6679\nFold1 Epoch [29/3000]: Train loss: 0.1025, Valid loss: 0.1122\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7374\nFold1 Epoch [30/3000]: Train loss: 0.1025, Valid loss: 0.1112\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7179\nFold1 Epoch [31/3000]: Train loss: 0.0988, Valid loss: 0.1065\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.6974\nFold1 Epoch [32/3000]: Train loss: 0.0984, Valid loss: 0.1083\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6979\nFold1 Epoch [33/3000]: Train loss: 0.1021, Valid loss: 0.1145\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.691\nFold1 Epoch [34/3000]: Train loss: 0.0993, Valid loss: 0.1094\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7026\nFold1 Epoch [35/3000]: Train loss: 0.0978, Valid loss: 0.1127\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6879\nFold1 Epoch [36/3000]: Train loss: 0.0989, Valid loss: 0.1109\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7356\nFold1 Epoch [37/3000]: Train loss: 0.0999, Valid loss: 0.1063\nacc: 0.8674, tpr: 0.16, tnr: 0.9808, ppv: 0.5714, npv: 0.8793, auc: 0.7264\nFold1 Epoch [38/3000]: Train loss: 0.0982, Valid loss: 0.1189\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7233\nFold1 Epoch [39/3000]: Train loss: 0.1129, Valid loss: 0.1045\nFold1 Saving model with loss 0.105...\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6967\nFold1 Epoch [40/3000]: Train loss: 0.0977, Valid loss: 0.1129\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7221\nFold1 Epoch [41/3000]: Train loss: 0.0989, Valid loss: 0.1175\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6856\nFold1 Epoch [42/3000]: Train loss: 0.1320, Valid loss: 0.1107\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.6897\nFold1 Epoch [43/3000]: Train loss: 0.1081, Valid loss: 0.1111\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6838\nFold1 Epoch [44/3000]: Train loss: 0.1060, Valid loss: 0.1104\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6979\nFold1 Epoch [45/3000]: Train loss: 0.0997, Valid loss: 0.1158\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7018\nFold1 Epoch [46/3000]: Train loss: 0.1001, Valid loss: 0.1068\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7038\nFold1 Epoch [47/3000]: Train loss: 0.0996, Valid loss: 0.1111\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7141\nFold1 Epoch [48/3000]: Train loss: 0.0935, Valid loss: 0.1051\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.72\nFold1 Epoch [49/3000]: Train loss: 0.0954, Valid loss: 0.1116\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.759\nFold1 Epoch [50/3000]: Train loss: 0.1008, Valid loss: 0.1052\nFold1 Saving model with auc 0.759...\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7154\nFold1 Epoch [51/3000]: Train loss: 0.1004, Valid loss: 0.1052\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7079\nFold1 Epoch [52/3000]: Train loss: 0.0960, Valid loss: 0.1062\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7467\nFold1 Epoch [53/3000]: Train loss: 0.0955, Valid loss: 0.1026\nFold1 Saving model with loss 0.103...\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7454\nFold1 Epoch [54/3000]: Train loss: 0.0951, Valid loss: 0.1044\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7244\nFold1 Epoch [55/3000]: Train loss: 0.0914, Valid loss: 0.1065\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.719\nFold1 Epoch [56/3000]: Train loss: 0.0973, Valid loss: 0.1052\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7244\nFold1 Epoch [57/3000]: Train loss: 0.0938, Valid loss: 0.1051\nacc: 0.8674, tpr: 0.08, tnr: 0.9936, ppv: 0.6667, npv: 0.8708, auc: 0.7233\nFold1 Epoch [58/3000]: Train loss: 0.0903, Valid loss: 0.1037\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7154\nFold1 Epoch [59/3000]: Train loss: 0.0912, Valid loss: 0.1045\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7028\nFold1 Epoch [60/3000]: Train loss: 0.0933, Valid loss: 0.1053\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7141\nFold1 Epoch [61/3000]: Train loss: 0.0916, Valid loss: 0.1054\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7033\nFold1 Epoch [62/3000]: Train loss: 0.0881, Valid loss: 0.1067\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7562\nFold1 Epoch [63/3000]: Train loss: 0.0919, Valid loss: 0.0999\nFold1 Saving model with loss 0.100...\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7433\nFold1 Epoch [64/3000]: Train loss: 0.0908, Valid loss: 0.1049\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.6936\nFold1 Epoch [65/3000]: Train loss: 0.0924, Valid loss: 0.1087\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.75\nFold1 Epoch [66/3000]: Train loss: 0.0963, Valid loss: 0.1012\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7431\nFold1 Epoch [67/3000]: Train loss: 0.1038, Valid loss: 0.1097\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7123\nFold1 Epoch [68/3000]: Train loss: 0.0994, Valid loss: 0.1135\nacc: 0.8674, tpr: 0.08, tnr: 0.9936, ppv: 0.6667, npv: 0.8708, auc: 0.7141\nFold1 Epoch [69/3000]: Train loss: 0.0926, Valid loss: 0.1067\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7649\nFold1 Epoch [70/3000]: Train loss: 0.0965, Valid loss: 0.0987\nFold1 Saving model with auc 0.765...\nFold1 Saving model with loss 0.099...\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7444\nFold1 Epoch [71/3000]: Train loss: 0.0939, Valid loss: 0.1051\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7215\nFold1 Epoch [72/3000]: Train loss: 0.0928, Valid loss: 0.1102\nacc: 0.8674, tpr: 0.08, tnr: 0.9936, ppv: 0.6667, npv: 0.8708, auc: 0.7315\nFold1 Epoch [73/3000]: Train loss: 0.0944, Valid loss: 0.1020\nacc: 0.8508, tpr: 0.04, tnr: 0.9808, ppv: 0.25, npv: 0.8644, auc: 0.6977\nFold1 Epoch [74/3000]: Train loss: 0.0897, Valid loss: 0.1068\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7062\nFold1 Epoch [75/3000]: Train loss: 0.0892, Valid loss: 0.1089\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7487\nFold1 Epoch [76/3000]: Train loss: 0.0886, Valid loss: 0.0998\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7146\nFold1 Epoch [77/3000]: Train loss: 0.0870, Valid loss: 0.1071\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7113\nFold1 Epoch [78/3000]: Train loss: 0.0916, Valid loss: 0.1049\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.731\nFold1 Epoch [79/3000]: Train loss: 0.0948, Valid loss: 0.1059\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7485\nFold1 Epoch [80/3000]: Train loss: 0.0996, Valid loss: 0.1023\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7141\nFold1 Epoch [81/3000]: Train loss: 0.0988, Valid loss: 0.1232\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7526\nFold1 Epoch [82/3000]: Train loss: 0.1013, Valid loss: 0.1004\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7172\nFold1 Epoch [83/3000]: Train loss: 0.0938, Valid loss: 0.1089\nacc: 0.8674, tpr: 0.2, tnr: 0.9744, ppv: 0.5556, npv: 0.8837, auc: 0.7121\nFold1 Epoch [84/3000]: Train loss: 0.0918, Valid loss: 0.1116\nacc: 0.8674, tpr: 0.24, tnr: 0.9679, ppv: 0.5455, npv: 0.8882, auc: 0.7574\nFold1 Epoch [85/3000]: Train loss: 0.0973, Valid loss: 0.1089\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7141\nFold1 Epoch [86/3000]: Train loss: 0.0921, Valid loss: 0.1054\nacc: 0.8508, tpr: 0.04, tnr: 0.9808, ppv: 0.25, npv: 0.8644, auc: 0.7318\nFold1 Epoch [87/3000]: Train loss: 0.0908, Valid loss: 0.1049\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7359\nFold1 Epoch [88/3000]: Train loss: 0.0895, Valid loss: 0.1021\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.719\nFold1 Epoch [89/3000]: Train loss: 0.0893, Valid loss: 0.1060\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7197\nFold1 Epoch [90/3000]: Train loss: 0.0868, Valid loss: 0.1046\nacc: 0.8674, tpr: 0.08, tnr: 0.9936, ppv: 0.6667, npv: 0.8708, auc: 0.7533\nFold1 Epoch [91/3000]: Train loss: 0.0936, Valid loss: 0.1003\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7346\nFold1 Epoch [92/3000]: Train loss: 0.0952, Valid loss: 0.1130\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7218\nFold1 Epoch [93/3000]: Train loss: 0.0906, Valid loss: 0.1058\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7131\nFold1 Epoch [94/3000]: Train loss: 0.0886, Valid loss: 0.1055\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7321\nFold1 Epoch [95/3000]: Train loss: 0.0922, Valid loss: 0.1060\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7303\nFold1 Epoch [96/3000]: Train loss: 0.0894, Valid loss: 0.1043\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.72\nFold1 Epoch [97/3000]: Train loss: 0.0863, Valid loss: 0.1034\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.74\nFold1 Epoch [98/3000]: Train loss: 0.0877, Valid loss: 0.1023\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.6992\nFold1 Epoch [99/3000]: Train loss: 0.0877, Valid loss: 0.1123\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7272\nFold1 Epoch [100/3000]: Train loss: 0.0904, Valid loss: 0.1023\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7164\nFold1 Epoch [101/3000]: Train loss: 0.0883, Valid loss: 0.1059\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7526\nFold1 Epoch [102/3000]: Train loss: 0.0909, Valid loss: 0.1000\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.741\nFold1 Epoch [103/3000]: Train loss: 0.0902, Valid loss: 0.1107\nacc: 0.8564, tpr: 0.2, tnr: 0.9615, ppv: 0.4545, npv: 0.8824, auc: 0.7564\nFold1 Epoch [104/3000]: Train loss: 0.1031, Valid loss: 0.1017\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7008\nFold1 Epoch [105/3000]: Train loss: 0.0890, Valid loss: 0.1162\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.759\nFold1 Epoch [106/3000]: Train loss: 0.0873, Valid loss: 0.0986\nFold1 Saving model with loss 0.099...\nacc: 0.8674, tpr: 0.08, tnr: 0.9936, ppv: 0.6667, npv: 0.8708, auc: 0.7092\nFold1 Epoch [107/3000]: Train loss: 0.0863, Valid loss: 0.1080\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7318\nFold1 Epoch [108/3000]: Train loss: 0.0872, Valid loss: 0.1073\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7469\nFold1 Epoch [109/3000]: Train loss: 0.0940, Valid loss: 0.1024\nacc: 0.8619, tpr: 0.16, tnr: 0.9744, ppv: 0.5, npv: 0.8786, auc: 0.7533\nFold1 Epoch [110/3000]: Train loss: 0.0929, Valid loss: 0.1037\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7449\nFold1 Epoch [111/3000]: Train loss: 0.0886, Valid loss: 0.1042\nacc: 0.8674, tpr: 0.24, tnr: 0.9679, ppv: 0.5455, npv: 0.8882, auc: 0.7215\nFold1 Epoch [112/3000]: Train loss: 0.0870, Valid loss: 0.1136\nacc: 0.8508, tpr: 0.12, tnr: 0.9679, ppv: 0.375, npv: 0.8728, auc: 0.7536\nFold1 Epoch [113/3000]: Train loss: 0.0891, Valid loss: 0.1024\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7151\nFold1 Epoch [114/3000]: Train loss: 0.0824, Valid loss: 0.1085\nacc: 0.8564, tpr: 0.24, tnr: 0.9551, ppv: 0.4615, npv: 0.8869, auc: 0.7097\nFold1 Epoch [115/3000]: Train loss: 0.0851, Valid loss: 0.1140\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7267\nFold1 Epoch [116/3000]: Train loss: 0.0847, Valid loss: 0.1035\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6895\nFold1 Epoch [117/3000]: Train loss: 0.0907, Valid loss: 0.1172\nacc: 0.8398, tpr: 0.08, tnr: 0.9615, ppv: 0.25, npv: 0.8671, auc: 0.7315\nFold1 Epoch [118/3000]: Train loss: 0.0987, Valid loss: 0.1083\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7192\nFold1 Epoch [119/3000]: Train loss: 0.0978, Valid loss: 0.1150\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7374\nFold1 Epoch [120/3000]: Train loss: 0.0935, Valid loss: 0.1019\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7382\nFold1 Epoch [121/3000]: Train loss: 0.0890, Valid loss: 0.1026\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7333\nFold1 Epoch [122/3000]: Train loss: 0.0842, Valid loss: 0.1058\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7044\nFold1 Epoch [123/3000]: Train loss: 0.0875, Valid loss: 0.1100\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7354\nFold1 Epoch [124/3000]: Train loss: 0.0834, Valid loss: 0.1028\nacc: 0.8564, tpr: 0.2, tnr: 0.9615, ppv: 0.4545, npv: 0.8824, auc: 0.7328\nFold1 Epoch [125/3000]: Train loss: 0.0860, Valid loss: 0.1128\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7572\nFold1 Epoch [126/3000]: Train loss: 0.0927, Valid loss: 0.1001\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7333\nFold1 Epoch [127/3000]: Train loss: 0.0900, Valid loss: 0.1046\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7438\nFold1 Epoch [128/3000]: Train loss: 0.0851, Valid loss: 0.1018\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7397\nFold1 Epoch [129/3000]: Train loss: 0.0838, Valid loss: 0.1040\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7205\nFold1 Epoch [130/3000]: Train loss: 0.0872, Valid loss: 0.1052\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7197\nFold1 Epoch [131/3000]: Train loss: 0.0853, Valid loss: 0.1039\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7133\nFold1 Epoch [132/3000]: Train loss: 0.0850, Valid loss: 0.1056\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7354\nFold1 Epoch [133/3000]: Train loss: 0.0836, Valid loss: 0.1036\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7174\nFold1 Epoch [134/3000]: Train loss: 0.0886, Valid loss: 0.1127\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7482\nFold1 Epoch [135/3000]: Train loss: 0.0899, Valid loss: 0.1025\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7436\nFold1 Epoch [136/3000]: Train loss: 0.0910, Valid loss: 0.1096\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.749\nFold1 Epoch [137/3000]: Train loss: 0.0870, Valid loss: 0.1010\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7413\nFold1 Epoch [138/3000]: Train loss: 0.0858, Valid loss: 0.1023\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7405\nFold1 Epoch [139/3000]: Train loss: 0.0861, Valid loss: 0.1069\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7495\nFold1 Epoch [140/3000]: Train loss: 0.0860, Valid loss: 0.1017\nacc: 0.8564, tpr: 0.24, tnr: 0.9551, ppv: 0.4615, npv: 0.8869, auc: 0.7246\nFold1 Epoch [141/3000]: Train loss: 0.0877, Valid loss: 0.1150\nacc: 0.8674, tpr: 0.24, tnr: 0.9679, ppv: 0.5455, npv: 0.8882, auc: 0.7503\nFold1 Epoch [142/3000]: Train loss: 0.0868, Valid loss: 0.1045\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7079\nFold1 Epoch [143/3000]: Train loss: 0.0826, Valid loss: 0.1120\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7621\nFold1 Epoch [144/3000]: Train loss: 0.0852, Valid loss: 0.1003\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.73\nFold1 Epoch [145/3000]: Train loss: 0.0847, Valid loss: 0.1048\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7272\nFold1 Epoch [146/3000]: Train loss: 0.0849, Valid loss: 0.1053\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7433\nFold1 Epoch [147/3000]: Train loss: 0.0868, Valid loss: 0.1087\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7126\nFold1 Epoch [148/3000]: Train loss: 0.0876, Valid loss: 0.1066\nacc: 0.8674, tpr: 0.2, tnr: 0.9744, ppv: 0.5556, npv: 0.8837, auc: 0.7336\nFold1 Epoch [149/3000]: Train loss: 0.0870, Valid loss: 0.1060\nacc: 0.8564, tpr: 0.28, tnr: 0.9487, ppv: 0.4667, npv: 0.8916, auc: 0.7592\nFold1 Epoch [150/3000]: Train loss: 0.0901, Valid loss: 0.1117\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7092\nFold1 Epoch [151/3000]: Train loss: 0.0875, Valid loss: 0.1034\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.6967\nFold1 Epoch [152/3000]: Train loss: 0.0860, Valid loss: 0.1061\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7374\nFold1 Epoch [153/3000]: Train loss: 0.0843, Valid loss: 0.1043\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7205\nFold1 Epoch [154/3000]: Train loss: 0.0827, Valid loss: 0.1057\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7215\nFold1 Epoch [155/3000]: Train loss: 0.0829, Valid loss: 0.1020\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7521\nFold1 Epoch [156/3000]: Train loss: 0.0868, Valid loss: 0.1026\nacc: 0.8674, tpr: 0.16, tnr: 0.9808, ppv: 0.5714, npv: 0.8793, auc: 0.7282\nFold1 Epoch [157/3000]: Train loss: 0.0848, Valid loss: 0.1026\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7392\nFold1 Epoch [158/3000]: Train loss: 0.0879, Valid loss: 0.1022\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7733\nFold1 Epoch [159/3000]: Train loss: 0.0866, Valid loss: 0.0993\nFold1 Saving model with auc 0.773...\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7064\nFold1 Epoch [160/3000]: Train loss: 0.0854, Valid loss: 0.1151\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7403\nFold1 Epoch [161/3000]: Train loss: 0.0866, Valid loss: 0.1019\nacc: 0.8453, tpr: 0.08, tnr: 0.9679, ppv: 0.2857, npv: 0.8678, auc: 0.7064\nFold1 Epoch [162/3000]: Train loss: 0.0832, Valid loss: 0.1088\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7605\nFold1 Epoch [163/3000]: Train loss: 0.0834, Valid loss: 0.1000\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7036\nFold1 Epoch [164/3000]: Train loss: 0.0893, Valid loss: 0.1097\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.751\nFold1 Epoch [165/3000]: Train loss: 0.0825, Valid loss: 0.1017\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7149\nFold1 Epoch [166/3000]: Train loss: 0.0812, Valid loss: 0.1067\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7431\nFold1 Epoch [167/3000]: Train loss: 0.0841, Valid loss: 0.1031\nacc: 0.8508, tpr: 0.04, tnr: 0.9808, ppv: 0.25, npv: 0.8644, auc: 0.7149\nFold1 Epoch [168/3000]: Train loss: 0.0814, Valid loss: 0.1042\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.739\nFold1 Epoch [169/3000]: Train loss: 0.0792, Valid loss: 0.1016\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7226\nFold1 Epoch [170/3000]: Train loss: 0.0797, Valid loss: 0.1057\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7385\nFold1 Epoch [171/3000]: Train loss: 0.0844, Valid loss: 0.1020\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7431\nFold1 Epoch [172/3000]: Train loss: 0.0862, Valid loss: 0.1032\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.72\nFold1 Epoch [173/3000]: Train loss: 0.0814, Valid loss: 0.1156\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7185\nFold1 Epoch [174/3000]: Train loss: 0.0871, Valid loss: 0.1035\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7136\nFold1 Epoch [175/3000]: Train loss: 0.0821, Valid loss: 0.1015\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7028\nFold1 Epoch [176/3000]: Train loss: 0.0797, Valid loss: 0.1055\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7626\nFold1 Epoch [177/3000]: Train loss: 0.0791, Valid loss: 0.1005\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.7059\nFold1 Epoch [178/3000]: Train loss: 0.0831, Valid loss: 0.1122\nacc: 0.8619, tpr: 0.2, tnr: 0.9679, ppv: 0.5, npv: 0.883, auc: 0.7551\nFold1 Epoch [179/3000]: Train loss: 0.0787, Valid loss: 0.0989\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7023\nFold1 Epoch [180/3000]: Train loss: 0.0803, Valid loss: 0.1067\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7636\nFold1 Epoch [181/3000]: Train loss: 0.0815, Valid loss: 0.0971\nFold1 Saving model with loss 0.097...\nacc: 0.8564, tpr: 0.2, tnr: 0.9615, ppv: 0.4545, npv: 0.8824, auc: 0.6995\nFold1 Epoch [182/3000]: Train loss: 0.0829, Valid loss: 0.1086\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7269\nFold1 Epoch [183/3000]: Train loss: 0.0819, Valid loss: 0.1070\nacc: 0.8508, tpr: 0.04, tnr: 0.9808, ppv: 0.25, npv: 0.8644, auc: 0.7467\nFold1 Epoch [184/3000]: Train loss: 0.0830, Valid loss: 0.1033\nacc: 0.8674, tpr: 0.08, tnr: 0.9936, ppv: 0.6667, npv: 0.8708, auc: 0.7633\nFold1 Epoch [185/3000]: Train loss: 0.0841, Valid loss: 0.1024\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.689\nFold1 Epoch [186/3000]: Train loss: 0.0842, Valid loss: 0.1103\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7482\nFold1 Epoch [187/3000]: Train loss: 0.0885, Valid loss: 0.1050\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.719\nFold1 Epoch [188/3000]: Train loss: 0.0863, Valid loss: 0.1076\nacc: 0.8674, tpr: 0.12, tnr: 0.9872, ppv: 0.6, npv: 0.875, auc: 0.7167\nFold1 Epoch [189/3000]: Train loss: 0.0822, Valid loss: 0.1025\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7285\nFold1 Epoch [190/3000]: Train loss: 0.0821, Valid loss: 0.1017\nacc: 0.8508, tpr: 0.2, tnr: 0.9551, ppv: 0.4167, npv: 0.8817, auc: 0.7179\nFold1 Epoch [191/3000]: Train loss: 0.0802, Valid loss: 0.1092\nacc: 0.8508, tpr: 0.12, tnr: 0.9679, ppv: 0.375, npv: 0.8728, auc: 0.7172\nFold1 Epoch [192/3000]: Train loss: 0.0792, Valid loss: 0.1077\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.71\nFold1 Epoch [193/3000]: Train loss: 0.0804, Valid loss: 0.1054\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7518\nFold1 Epoch [194/3000]: Train loss: 0.0851, Valid loss: 0.0999\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.6867\nFold1 Epoch [195/3000]: Train loss: 0.0860, Valid loss: 0.1130\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7387\nFold1 Epoch [196/3000]: Train loss: 0.0819, Valid loss: 0.1021\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.6913\nFold1 Epoch [197/3000]: Train loss: 0.0791, Valid loss: 0.1085\nacc: 0.8508, tpr: 0.16, tnr: 0.9615, ppv: 0.4, npv: 0.8772, auc: 0.7487\nFold1 Epoch [198/3000]: Train loss: 0.0807, Valid loss: 0.1041\nacc: 0.8508, tpr: 0.28, tnr: 0.9423, ppv: 0.4375, npv: 0.8909, auc: 0.7374\nFold1 Epoch [199/3000]: Train loss: 0.0806, Valid loss: 0.1132\nacc: 0.8453, tpr: 0.12, tnr: 0.9615, ppv: 0.3333, npv: 0.8721, auc: 0.7336\nFold1 Epoch [200/3000]: Train loss: 0.0831, Valid loss: 0.1087\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.7313\nFold1 Epoch [201/3000]: Train loss: 0.0830, Valid loss: 0.1011\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.6723\nFold1 Epoch [202/3000]: Train loss: 0.0779, Valid loss: 0.1145\nacc: 0.8674, tpr: 0.04, tnr: 1.0, ppv: 1.0, npv: 0.8667, auc: 0.6977\nFold1 Epoch [203/3000]: Train loss: 0.0855, Valid loss: 0.1117\nacc: 0.8508, tpr: 0.12, tnr: 0.9679, ppv: 0.375, npv: 0.8728, auc: 0.7231\nFold1 Epoch [204/3000]: Train loss: 0.0933, Valid loss: 0.1034\nacc: 0.8619, tpr: 0.2, tnr: 0.9679, ppv: 0.5, npv: 0.883, auc: 0.7346\nFold1 Epoch [205/3000]: Train loss: 0.0898, Valid loss: 0.1074\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7387\nFold1 Epoch [206/3000]: Train loss: 0.0841, Valid loss: 0.1026\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7092\nFold1 Epoch [207/3000]: Train loss: 0.0856, Valid loss: 0.1033\nacc: 0.8729, tpr: 0.08, tnr: 1.0, ppv: 1.0, npv: 0.8715, auc: 0.6692\nFold1 Epoch [208/3000]: Train loss: 0.0820, Valid loss: 0.1067\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7523\nFold1 Epoch [209/3000]: Train loss: 0.0827, Valid loss: 0.0994\nacc: 0.8453, tpr: 0.08, tnr: 0.9679, ppv: 0.2857, npv: 0.8678, auc: 0.7174\nFold1 Epoch [210/3000]: Train loss: 0.0747, Valid loss: 0.1026\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7049\nFold1 Epoch [211/3000]: Train loss: 0.0807, Valid loss: 0.1052\nacc: 0.8564, tpr: 0.08, tnr: 0.9808, ppv: 0.4, npv: 0.8693, auc: 0.6928\nFold1 Epoch [212/3000]: Train loss: 0.0819, Valid loss: 0.1051\nacc: 0.8619, tpr: 0.04, tnr: 0.9936, ppv: 0.5, npv: 0.8659, auc: 0.7192\nFold1 Epoch [213/3000]: Train loss: 0.0872, Valid loss: 0.1069\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7351\nFold1 Epoch [214/3000]: Train loss: 0.0837, Valid loss: 0.1021\nacc: 0.8508, tpr: 0.04, tnr: 0.9808, ppv: 0.25, npv: 0.8644, auc: 0.7128\nFold1 Epoch [215/3000]: Train loss: 0.0816, Valid loss: 0.1067\nacc: 0.8674, tpr: 0.08, tnr: 0.9936, ppv: 0.6667, npv: 0.8708, auc: 0.6946\nFold1 Epoch [216/3000]: Train loss: 0.0903, Valid loss: 0.1074\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7295\nFold1 Epoch [217/3000]: Train loss: 0.0905, Valid loss: 0.1077\nacc: 0.8729, tpr: 0.08, tnr: 1.0, ppv: 1.0, npv: 0.8715, auc: 0.6895\nFold1 Epoch [218/3000]: Train loss: 0.0861, Valid loss: 0.1107\nacc: 0.8564, tpr: 0.16, tnr: 0.9679, ppv: 0.4444, npv: 0.8779, auc: 0.7585\nFold1 Epoch [219/3000]: Train loss: 0.0841, Valid loss: 0.0997\nacc: 0.8564, tpr: 0.04, tnr: 0.9872, ppv: 0.3333, npv: 0.8652, auc: 0.7021\nFold1 Epoch [220/3000]: Train loss: 0.0807, Valid loss: 0.1095\nacc: 0.8619, tpr: 0.12, tnr: 0.9808, ppv: 0.5, npv: 0.8743, auc: 0.7092\nFold1 Epoch [221/3000]: Train loss: 0.0815, Valid loss: 0.1015\nacc: 0.8508, tpr: 0.16, tnr: 0.9615, ppv: 0.4, npv: 0.8772, auc: 0.7131\nFold1 Epoch [222/3000]: Train loss: 0.0770, Valid loss: 0.1036\nacc: 0.8619, tpr: 0.08, tnr: 0.9872, ppv: 0.5, npv: 0.8701, auc: 0.7054\nFold1 Epoch [223/3000]: Train loss: 0.0784, Valid loss: 0.1031\nacc: 0.8508, tpr: 0.08, tnr: 0.9744, ppv: 0.3333, npv: 0.8686, auc: 0.7203\nFold1 Epoch [224/3000]: Train loss: 0.0806, Valid loss: 0.1054\nacc: 0.8564, tpr: 0.2, tnr: 0.9615, ppv: 0.4545, npv: 0.8824, auc: 0.7008\nFold1 Epoch [225/3000]: Train loss: 0.0791, Valid loss: 0.1079\nacc: 0.8729, tpr: 0.28, tnr: 0.9679, ppv: 0.5833, npv: 0.8935, auc: 0.7664\nFold1 Epoch [226/3000]: Train loss: 0.0799, Valid loss: 0.1042\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.6723\nFold1 Epoch [227/3000]: Train loss: 0.0751, Valid loss: 0.1154\nacc: 0.8564, tpr: 0.12, tnr: 0.9744, ppv: 0.4286, npv: 0.8736, auc: 0.7038\nFold1 Epoch [228/3000]: Train loss: 0.0766, Valid loss: 0.1061\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[74], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m valloader \u001b[38;5;241m=\u001b[39m DataLoader(valset, batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trainset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# put your model and data on the same computation device.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[72], line 50\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(train_loader, valid_loader, model, cfg, device, k, model_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m valid_loader:\n\u001b[1;32m     51\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[68], line 61\u001b[0m, in \u001b[0;36mRTDataset.__getitem__\u001b[0;34m(self, origin_idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, origin_idx):\n\u001b[1;32m     60\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_map[origin_idx] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m     clinical_values \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# label\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_data[idx]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import requests\n\nrequests.get('https://hooks.zapier.com/hooks/catch/18160905/3cvu8pz/')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Model(input_dim=len(train_dataset.__getitem__(0)[0])).to(device)\n# model.load_state_dict(torch.load(cfg.save_path))\n# preds = predict(test_loader, model, device) \n# save_pred(preds, cfg.pred_path)         ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:14.617713Z","iopub.status.idle":"2024-03-18T10:44:14.618066Z","shell.execute_reply.started":"2024-03-18T10:44:14.617889Z","shell.execute_reply":"2024-03-18T10:44:14.617904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Distribution","metadata":{}},{"cell_type":"code","source":"# test_data = pd.read_csv(cfg.valid_path).values\n# y42 = test_data[:, -3]\n# y42_unique, y42_counts = np.unique(y42, return_counts=True)\n# # dict(zip(unique, counts))\n# # np.asarray((unique, counts)).T\n# y90 = test_data[:, -2]\n# y90_unique, y90_counts = np.unique(y90, return_counts=True)\n# y365 = test_data[:, -1]\n# y365_unique, y365_counts = np.unique(y365, return_counts=True)\n# print(\n#     y42, dict(zip(y42_unique, y42_counts)),\n#     y90, dict(zip(y90_unique, y90_counts)),\n#     y365, dict(zip(y365_unique, y365_counts))\n# )","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:14.619126Z","iopub.status.idle":"2024-03-18T10:44:14.619477Z","shell.execute_reply.started":"2024-03-18T10:44:14.619306Z","shell.execute_reply":"2024-03-18T10:44:14.619322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_scorer = sklearn.metrics.get_scorer_names()\n# print(all_scorer)\n\ndef evaluate_preds(preds, y_test, threshold=0.5):\n    _preds = []\n    for i, v in enumerate(preds):\n        _preds.append(1 if (v > threshold) else 0)\n\n    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_test, _preds).ravel()\n    print(tn, fp, fn, tp)\n\n    acc = (tp+tn)/(tn+fp+fn+tp)\n    # acc = sklearn.metrics.accuracy_score(y_test, _preds)\n\n    sensitivity = tp/(tp+fn) # Recall\n    # sensitivity = sklearn.metrics.recall_score(y_test, _preds)\n\n    precision = tp/(tp+fp) # Precision & ppv\n    # precision = sklearn.metrics.precision_score(y_test, _preds)\n\n    npv = tn/(tn+fn)\n\n    specificity = tn/(tn+fp)\n\n    try:\n        f1 = ((2*precision*sensitivity)/(precision+sensitivity))\n    except:\n        f1 = 0\n    # f1 = sklearn.metrics.f1_score(y_test, _preds)\n\n    auc = sklearn.metrics.roc_auc_score(y_test, preds)\n\n    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, preds)\n    print(fpr)\n    print(tpr)\n    print(thresholds)\n    roc_auc = sklearn.metrics.auc(fpr, tpr)\n    display = sklearn.metrics.RocCurveDisplay(\n        fpr=fpr,\n        tpr=tpr,\n        roc_auc=roc_auc\n    )\n    display.plot()\n    plt.show()\n\n    print(f'model_name: {cfg.model_name}')\n    print(f'acc: {acc}\\nsensitivity: {sensitivity}\\nspecificity: {specificity}\\nppv(precision): {precision}\\nnpv: {npv}\\nF1-score: {f1}\\nAUC: {auc}')","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:14.620892Z","iopub.status.idle":"2024-03-18T10:44:14.621342Z","shell.execute_reply.started":"2024-03-18T10:44:14.621114Z","shell.execute_reply":"2024-03-18T10:44:14.621134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import normalize\nimport matplotlib.pyplot as plt\n\n# evaluate_preds(preds, y_test, 0.6597)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:14.622396Z","iopub.status.idle":"2024-03-18T10:44:14.622837Z","shell.execute_reply.started":"2024-03-18T10:44:14.622600Z","shell.execute_reply":"2024-03-18T10:44:14.622619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# |sensitivity - specificity| < 0.1\n#  model \n# ppv, npv, F1-score ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:44:14.625000Z","iopub.status.idle":"2024-03-18T10:44:14.625451Z","shell.execute_reply.started":"2024-03-18T10:44:14.625224Z","shell.execute_reply":"2024-03-18T10:44:14.625244Z"},"trusted":true},"execution_count":null,"outputs":[]}]}